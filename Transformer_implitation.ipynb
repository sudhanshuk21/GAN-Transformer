{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Transformers\n"
      ],
      "metadata": {
        "id": "cGPfh7qplaxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install BPEmb\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from bpemb import BPEmb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk5VEGhQlhUU",
        "outputId": "8d2536ec-f962-4882-deb3-9e668f7f1c01"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting BPEmb\n",
            "  Downloading bpemb-0.3.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from BPEmb) (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from BPEmb) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from BPEmb) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from BPEmb) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from BPEmb) (4.66.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->BPEmb) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->BPEmb) (7.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->BPEmb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->BPEmb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->BPEmb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->BPEmb) (2024.7.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->BPEmb) (1.14.1)\n",
            "Downloading bpemb-0.3.5-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: BPEmb\n",
            "Successfully installed BPEmb-0.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers From Scratch"
      ],
      "metadata": {
        "id": "IGiI2aLkmEqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Self-Attention"
      ],
      "metadata": {
        "id": "yKmdGUSymPU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  key_dim = tf.cast(tf.shape(key)[-1],tf.float32)\n",
        "  scaled_scores = tf.matmul(query, key, transpose_b=True)/np.sqrt(key_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
        "\n",
        "  softmax = tf.keras.layers.Softmax()\n",
        "  weights = softmax(scaled_scores)\n",
        "  return tf.matmul(weights, value),weights"
      ],
      "metadata": {
        "id": "tXDfLsjYmH5w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose our queries, keys, and values are each a length of 3 with a dimension of 4."
      ],
      "metadata": {
        "id": "9UI9Nj8MoH4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len =3\n",
        "embed_dim =4\n",
        "\n",
        "queries = np.random.rand(seq_len, embed_dim)\n",
        "keys = np.random.rand(seq_len, embed_dim)\n",
        "values =  np.random.rand(seq_len, embed_dim)\n",
        "\n",
        "print(\"Queries:\\n\",queries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QcRossRmi9A",
        "outputId": "3cf24aef-1637-42c4-9261-4a27362a2246"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries:\n",
            " [[0.82788891 0.47845084 0.15714683 0.87821002]\n",
            " [0.64513255 0.17505266 0.11050864 0.83509783]\n",
            " [0.50878368 0.85307375 0.14718729 0.17733292]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output, attn_weights = scaled_dot_product_attention(queries,keys,values)\n",
        "\n",
        "print(\"Output\\n\", output, \"\\n\")\n",
        "print(\"Weights\\n\", attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVp-0UUyoPmT",
        "outputId": "8484e71b-93a1-4363-f823-1cbf437e4c3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output\n",
            " tf.Tensor(\n",
            "[[0.7369056  0.77670056 0.6728094  0.70853746]\n",
            " [0.73682296 0.7711015  0.65910083 0.7067499 ]\n",
            " [0.7405319  0.77426565 0.6692036  0.7087018 ]], shape=(3, 4), dtype=float32) \n",
            "\n",
            "Weights\n",
            " tf.Tensor(\n",
            "[[0.40532133 0.3466487  0.24803   ]\n",
            " [0.3807964  0.34614053 0.27306303]\n",
            " [0.4037669  0.33936718 0.25686586]], shape=(3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating queries, keys, and values for multiple heads."
      ],
      "metadata": {
        "id": "wh3olD_gpoOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "seq_len = 3\n",
        "embed_dim = 12\n",
        "num_heads = 3\n",
        "head_dim = embed_dim // num_heads\n",
        "\n",
        "print(f\"Dimension of each head: {head_dim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhcJEJRZpipg",
        "outputId": "a220d833-bd99-4a0a-cb2d-c72902561d44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of each head: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.rand(batch_size, seq_len, embed_dim).round(1)\n",
        "print(\"Input shape: \", x.shape, \"\\n\")\n",
        "print(\"Input:\\n\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNa7zQ9Ip8eZ",
        "outputId": "453d611a-bb49-45c2-b7aa-168bda24eb7d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  (1, 3, 12) \n",
            "\n",
            "Input:\n",
            " [[[0.8 0.6 0.1 0.9 0.3 1.  0.7 0.2 0.  0.4 0.3 0.9]\n",
            "  [0.  1.  0.2 0.2 0.4 0.7 0.5 0.3 0.9 0.3 0.8 0.3]\n",
            "  [0.5 0.3 0.6 0.9 0.9 0.7 0.  0.9 0.2 0.2 0.2 0.4]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The query weights for each head.\n",
        "wq0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wq1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wq2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "# The key weights for each head.\n",
        "wk0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wk1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wk2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "# The value weights for each head.\n",
        "wv0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wv1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wv2 = np.random.rand(embed_dim, head_dim).round(1)"
      ],
      "metadata": {
        "id": "YhhtmbPEqE0F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The three sets of query weights (one for each head):\")\n",
        "print(\"wq0:\\n\", wq0)\n",
        "print(\"wq1:\\n\", wq1)\n",
        "print(\"wq2:\\n\", wq1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7SiUfLYqJaV",
        "outputId": "a20338fa-ec20-4b53-dd22-4f2cef640507"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The three sets of query weights (one for each head):\n",
            "wq0:\n",
            " [[0.7 0.5 0.9 0.1]\n",
            " [0.3 0.4 0.7 0.9]\n",
            " [0.2 0.1 0.7 0.3]\n",
            " [0.5 0.7 0.1 0.7]\n",
            " [0.2 0.5 0.3 0.7]\n",
            " [0.2 0.4 0.2 0.5]\n",
            " [0.6 0.7 0.3 0.4]\n",
            " [0.7 0.2 0.4 0.8]\n",
            " [0.8 0.7 0.6 0.1]\n",
            " [0.7 0.5 0.9 0.7]\n",
            " [0.5 0.4 0.1 0.2]\n",
            " [0.8 1.  0.9 0.8]]\n",
            "wq1:\n",
            " [[0.3 0.2 0.7 0.6]\n",
            " [0.1 0.9 0.1 0.8]\n",
            " [0.5 0.2 0.8 0.7]\n",
            " [0.  0.7 0.1 0.4]\n",
            " [0.6 0.6 0.6 0.2]\n",
            " [0.2 0.7 0.4 0.5]\n",
            " [0.3 0.4 0.2 0.4]\n",
            " [0.6 0.2 0.2 0.7]\n",
            " [0.7 0.7 0.2 0.6]\n",
            " [0.9 0.7 0.7 0.4]\n",
            " [0.6 0.2 0.1 0.9]\n",
            " [0.1 0.8 0.4 0.9]]\n",
            "wq2:\n",
            " [[0.3 0.2 0.7 0.6]\n",
            " [0.1 0.9 0.1 0.8]\n",
            " [0.5 0.2 0.8 0.7]\n",
            " [0.  0.7 0.1 0.4]\n",
            " [0.6 0.6 0.6 0.2]\n",
            " [0.2 0.7 0.4 0.5]\n",
            " [0.3 0.4 0.2 0.4]\n",
            " [0.6 0.2 0.2 0.7]\n",
            " [0.7 0.7 0.2 0.6]\n",
            " [0.9 0.7 0.7 0.4]\n",
            " [0.6 0.2 0.1 0.9]\n",
            " [0.1 0.8 0.4 0.9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Geneated queries, keys, and values for the first head.\n",
        "q0 = np.dot(x, wq0)\n",
        "k0 = np.dot(x, wk0)\n",
        "v0 = np.dot(x, wv0)\n",
        "\n",
        "# Geneated queries, keys, and values for the second head.\n",
        "q1 = np.dot(x, wq1)\n",
        "k1 = np.dot(x, wk1)\n",
        "v1 = np.dot(x, wv1)\n",
        "\n",
        "# Geneated queries, keys, and values for the third head.\n",
        "q2 = np.dot(x, wq2)\n",
        "k2 = np.dot(x, wk2)\n",
        "v2 = np.dot(x, wv2)"
      ],
      "metadata": {
        "id": "-RtrwX5EqNJh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q, K, and V for first head:\\n\")\n",
        "\n",
        "print(f\"q0 {q0.shape}:\\n\", q0, \"\\n\")\n",
        "print(f\"k0 {k0.shape}:\\n\", k0, \"\\n\")\n",
        "print(f\"v0 {v0.shape}:\\n\", v0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHr7ml3JqTyz",
        "outputId": "cf8f2982-d8fd-4038-ca8a-7343cc253f32"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q, K, and V for first head:\n",
            "\n",
            "q0 (1, 3, 4):\n",
            " [[[3.18 3.58 3.08 3.49]\n",
            "  [2.74 2.85 2.55 2.87]\n",
            "  [2.68 2.69 2.62 3.35]]] \n",
            "\n",
            "k0 (1, 3, 4):\n",
            " [[[2.82 3.94 3.59 3.23]\n",
            "  [2.42 3.07 3.3  2.83]\n",
            "  [2.09 3.73 3.11 3.1 ]]] \n",
            "\n",
            "v0 (1, 3, 4):\n",
            " [[[3.3  3.04 3.53 1.79]\n",
            "  [2.98 2.32 2.95 2.73]\n",
            "  [3.53 2.66 2.37 1.65]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out0, attn_weights0 = scaled_dot_product_attention(q0, k0, v0)\n",
        "\n",
        "print(\"Output from first attention head: \", out0, \"\\n\")\n",
        "print(\"Attention weights from first head: \", attn_weights0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nz-BHMiqYrJ",
        "outputId": "d12e60cd-bbb9-4452-d6ef-a6074c0a46b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first attention head:  tf.Tensor(\n",
            "[[[3.3066816 2.9892757 3.4265764 1.8096166]\n",
            "  [3.3062172 2.9612694 3.3783638 1.8270708]\n",
            "  [3.3064375 2.9624128 3.3799012 1.8260374]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "Attention weights from first head:  tf.Tensor(\n",
            "[[[0.8949512  0.03178101 0.0732677 ]\n",
            "  [0.8418675  0.05482367 0.10330877]\n",
            "  [0.8437963  0.05361672 0.10258693]]], shape=(1, 3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out0, attn_weights0 = scaled_dot_product_attention(q0, k0, v0)\n",
        "\n",
        "print(\"Output from first attention head: \", out0, \"\\n\")\n",
        "print(\"Attention weights from first head: \", attn_weights0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuCkgtVFqvqd",
        "outputId": "744d06a0-950a-4cbd-c169-015520101700"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first attention head:  tf.Tensor(\n",
            "[[[3.3066816 2.9892757 3.4265764 1.8096166]\n",
            "  [3.3062172 2.9612694 3.3783638 1.8270708]\n",
            "  [3.3064375 2.9624128 3.3799012 1.8260374]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "Attention weights from first head:  tf.Tensor(\n",
            "[[[0.8949512  0.03178101 0.0732677 ]\n",
            "  [0.8418675  0.05482367 0.10330877]\n",
            "  [0.8437963  0.05361672 0.10258693]]], shape=(1, 3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out1, _ = scaled_dot_product_attention(q1, k1, v1)\n",
        "out2, _ = scaled_dot_product_attention(q2, k2, v2)"
      ],
      "metadata": {
        "id": "XvbHUvYgq14Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_out_a = np.concatenate((out0, out1, out2), axis=-1)\n",
        "print(f\"Combined output from all heads {combined_out_a.shape}:\")\n",
        "print(combined_out_a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKl2r0ssq7FG",
        "outputId": "ade72f56-6089-41d7-81bd-8bd0952a8245"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined output from all heads (1, 3, 12):\n",
            "[[[3.3066816 2.9892757 3.4265764 1.8096166 3.6096988 2.9276514 2.1850147\n",
            "   3.0830379 2.6692083 2.739544  2.7760735 3.4935157]\n",
            "  [3.3062172 2.9612694 3.3783638 1.8270708 3.5816612 2.9478528 2.197434\n",
            "   3.0901368 2.7068827 2.7709622 2.7728791 3.4823875]\n",
            "  [3.3064375 2.9624128 3.3799012 1.8260374 3.5807714 2.9463234 2.191193\n",
            "   3.0957341 2.6996639 2.7612586 2.7779686 3.4790032]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Query weights for first head: \\n\", wq0, \"\\n\")\n",
        "print(\"Query weights for second head: \\n\", wq1, \"\\n\")\n",
        "print(\"Query weights for third head: \\n\", wq2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZdhz3yQq_Uy",
        "outputId": "2bd9faa3-68c3-451e-81ee-aef8547d2fcd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query weights for first head: \n",
            " [[0.7 0.5 0.9 0.1]\n",
            " [0.3 0.4 0.7 0.9]\n",
            " [0.2 0.1 0.7 0.3]\n",
            " [0.5 0.7 0.1 0.7]\n",
            " [0.2 0.5 0.3 0.7]\n",
            " [0.2 0.4 0.2 0.5]\n",
            " [0.6 0.7 0.3 0.4]\n",
            " [0.7 0.2 0.4 0.8]\n",
            " [0.8 0.7 0.6 0.1]\n",
            " [0.7 0.5 0.9 0.7]\n",
            " [0.5 0.4 0.1 0.2]\n",
            " [0.8 1.  0.9 0.8]] \n",
            "\n",
            "Query weights for second head: \n",
            " [[0.3 0.2 0.7 0.6]\n",
            " [0.1 0.9 0.1 0.8]\n",
            " [0.5 0.2 0.8 0.7]\n",
            " [0.  0.7 0.1 0.4]\n",
            " [0.6 0.6 0.6 0.2]\n",
            " [0.2 0.7 0.4 0.5]\n",
            " [0.3 0.4 0.2 0.4]\n",
            " [0.6 0.2 0.2 0.7]\n",
            " [0.7 0.7 0.2 0.6]\n",
            " [0.9 0.7 0.7 0.4]\n",
            " [0.6 0.2 0.1 0.9]\n",
            " [0.1 0.8 0.4 0.9]] \n",
            "\n",
            "Query weights for third head: \n",
            " [[0.2 0.5 0.6 0.8]\n",
            " [0.5 0.5 0.6 0.5]\n",
            " [0.8 0.4 0.9 0.4]\n",
            " [0.1 1.  0.2 0.1]\n",
            " [0.8 0.8 0.1 0.8]\n",
            " [0.9 0.2 0.2 0.8]\n",
            " [0.1 0.4 0.4 0. ]\n",
            " [0.2 0.3 0.  0.5]\n",
            " [0.7 0.4 0.5 0.6]\n",
            " [0.9 0.5 0.2 0.3]\n",
            " [0.7 0.  0.5 0.5]\n",
            " [1.  0.7 0.5 0.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Query weights for first head: \\n\", wq0, \"\\n\")\n",
        "print(\"Query weights for second head: \\n\", wq1, \"\\n\")\n",
        "print(\"Query weights for third head: \\n\", wq2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B0Nd4rWrFqO",
        "outputId": "7772f842-8b0a-431e-be8a-6697333188d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query weights for first head: \n",
            " [[0.7 0.5 0.9 0.1]\n",
            " [0.3 0.4 0.7 0.9]\n",
            " [0.2 0.1 0.7 0.3]\n",
            " [0.5 0.7 0.1 0.7]\n",
            " [0.2 0.5 0.3 0.7]\n",
            " [0.2 0.4 0.2 0.5]\n",
            " [0.6 0.7 0.3 0.4]\n",
            " [0.7 0.2 0.4 0.8]\n",
            " [0.8 0.7 0.6 0.1]\n",
            " [0.7 0.5 0.9 0.7]\n",
            " [0.5 0.4 0.1 0.2]\n",
            " [0.8 1.  0.9 0.8]] \n",
            "\n",
            "Query weights for second head: \n",
            " [[0.3 0.2 0.7 0.6]\n",
            " [0.1 0.9 0.1 0.8]\n",
            " [0.5 0.2 0.8 0.7]\n",
            " [0.  0.7 0.1 0.4]\n",
            " [0.6 0.6 0.6 0.2]\n",
            " [0.2 0.7 0.4 0.5]\n",
            " [0.3 0.4 0.2 0.4]\n",
            " [0.6 0.2 0.2 0.7]\n",
            " [0.7 0.7 0.2 0.6]\n",
            " [0.9 0.7 0.7 0.4]\n",
            " [0.6 0.2 0.1 0.9]\n",
            " [0.1 0.8 0.4 0.9]] \n",
            "\n",
            "Query weights for third head: \n",
            " [[0.2 0.5 0.6 0.8]\n",
            " [0.5 0.5 0.6 0.5]\n",
            " [0.8 0.4 0.9 0.4]\n",
            " [0.1 1.  0.2 0.1]\n",
            " [0.8 0.8 0.1 0.8]\n",
            " [0.9 0.2 0.2 0.8]\n",
            " [0.1 0.4 0.4 0. ]\n",
            " [0.2 0.3 0.  0.5]\n",
            " [0.7 0.4 0.5 0.6]\n",
            " [0.9 0.5 0.2 0.3]\n",
            " [0.7 0.  0.5 0.5]\n",
            " [1.  0.7 0.5 0.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wq = np.concatenate((wq0, wq1, wq2), axis=1)\n",
        "print(f\"Single query weight matrix {wq.shape}: \\n\", wq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06am1AuRrLAj",
        "outputId": "f99b47a7-37e4-41ad-d770-5c2b4be66a0e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single query weight matrix (12, 12): \n",
            " [[0.7 0.5 0.9 0.1 0.3 0.2 0.7 0.6 0.2 0.5 0.6 0.8]\n",
            " [0.3 0.4 0.7 0.9 0.1 0.9 0.1 0.8 0.5 0.5 0.6 0.5]\n",
            " [0.2 0.1 0.7 0.3 0.5 0.2 0.8 0.7 0.8 0.4 0.9 0.4]\n",
            " [0.5 0.7 0.1 0.7 0.  0.7 0.1 0.4 0.1 1.  0.2 0.1]\n",
            " [0.2 0.5 0.3 0.7 0.6 0.6 0.6 0.2 0.8 0.8 0.1 0.8]\n",
            " [0.2 0.4 0.2 0.5 0.2 0.7 0.4 0.5 0.9 0.2 0.2 0.8]\n",
            " [0.6 0.7 0.3 0.4 0.3 0.4 0.2 0.4 0.1 0.4 0.4 0. ]\n",
            " [0.7 0.2 0.4 0.8 0.6 0.2 0.2 0.7 0.2 0.3 0.  0.5]\n",
            " [0.8 0.7 0.6 0.1 0.7 0.7 0.2 0.6 0.7 0.4 0.5 0.6]\n",
            " [0.7 0.5 0.9 0.7 0.9 0.7 0.7 0.4 0.9 0.5 0.2 0.3]\n",
            " [0.5 0.4 0.1 0.2 0.6 0.2 0.1 0.9 0.7 0.  0.5 0.5]\n",
            " [0.8 1.  0.9 0.8 0.1 0.8 0.4 0.9 1.  0.7 0.5 0.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wk = np.concatenate((wk0, wk1, wk2), axis=1)\n",
        "wv = np.concatenate((wv0, wv1, wv2), axis=1)"
      ],
      "metadata": {
        "id": "i5bp3SbgrRD3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_s = np.dot(x, wq)\n",
        "k_s = np.dot(x, wk)\n",
        "v_s = np.dot(x, wv)"
      ],
      "metadata": {
        "id": "sw0yQTEqrSRa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Query vectors using a single weight matrix {q_s.shape}:\\n\", q_s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIrKF6OqrWM0",
        "outputId": "fc8c76ef-d94b-4632-a350-8e23416913f9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query vectors using a single weight matrix (1, 3, 12):\n",
            " [[[3.18 3.58 3.08 3.49 1.69 3.61 2.22 3.61 3.35 3.25 2.3  3.02]\n",
            "  [2.74 2.85 2.55 2.87 2.32 3.31 1.55 3.51 3.5  2.25 2.26 2.84]\n",
            "  [2.68 2.69 2.62 3.35 2.18 2.97 2.31 3.22 3.21 3.13 1.87 3.13]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_s_reshaped = tf.reshape(q_s, (batch_size, seq_len, num_heads, head_dim))"
      ],
      "metadata": {
        "id": "mZtXtuQDraA8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_s_transposed = tf.transpose(q_s_reshaped, perm=[0, 2, 1, 3]).numpy()"
      ],
      "metadata": {
        "id": "0XyZvs8Lrpb8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The separate per-head query matrices from before: \")\n",
        "print(q0, \"\\n\")\n",
        "print(q1, \"\\n\")\n",
        "print(q2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owcp-P5-rthR",
        "outputId": "16e1dfe3-6027-40e4-bc87-159dee2851fa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The separate per-head query matrices from before: \n",
            "[[[3.18 3.58 3.08 3.49]\n",
            "  [2.74 2.85 2.55 2.87]\n",
            "  [2.68 2.69 2.62 3.35]]] \n",
            "\n",
            "[[[1.69 3.61 2.22 3.61]\n",
            "  [2.32 3.31 1.55 3.51]\n",
            "  [2.18 2.97 2.31 3.22]]] \n",
            "\n",
            "[[[3.35 3.25 2.3  3.02]\n",
            "  [3.5  2.25 2.26 2.84]\n",
            "  [3.21 3.13 1.87 3.13]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_s_transposed = tf.transpose(tf.reshape(k_s, (batch_size, -1, num_heads, head_dim)), perm=[0, 2, 1, 3]).numpy()\n",
        "v_s_transposed = tf.transpose(tf.reshape(v_s, (batch_size, -1, num_heads, head_dim)), perm=[0, 2, 1, 3]).numpy()"
      ],
      "metadata": {
        "id": "YrKiBi_YrxMV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_heads_output, all_attn_weights = scaled_dot_product_attention(q_s_transposed,\n",
        "                                                                  k_s_transposed,\n",
        "                                                                  v_s_transposed)\n",
        "print(\"Self attention output:\\n\", all_heads_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI5Hgsmur281",
        "outputId": "447735e8-aa2b-41bd-c116-46a8d1b920e0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self attention output:\n",
            " tf.Tensor(\n",
            "[[[[3.3066816 2.9892757 3.4265764 1.8096166]\n",
            "   [3.3062172 2.9612694 3.3783638 1.8270708]\n",
            "   [3.3064375 2.9624128 3.3799012 1.8260374]]\n",
            "\n",
            "  [[3.6096988 2.9276514 2.1850147 3.0830379]\n",
            "   [3.5816612 2.9478528 2.197434  3.0901368]\n",
            "   [3.5807714 2.9463234 2.191193  3.0957341]]\n",
            "\n",
            "  [[2.6692083 2.739544  2.7760735 3.4935157]\n",
            "   [2.7068827 2.7709622 2.7728791 3.4823875]\n",
            "   [2.6996639 2.7612586 2.7779686 3.4790032]]]], shape=(1, 3, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Per head outputs from using separate sets of weights per head:\")\n",
        "print(out0, \"\\n\")\n",
        "print(out1, \"\\n\")\n",
        "print(out2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfsh34dDr-mi",
        "outputId": "3d2b6140-2ddb-43f2-a5dc-7a104ba6ae26"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per head outputs from using separate sets of weights per head:\n",
            "tf.Tensor(\n",
            "[[[3.3066816 2.9892757 3.4265764 1.8096166]\n",
            "  [3.3062172 2.9612694 3.3783638 1.8270708]\n",
            "  [3.3064375 2.9624128 3.3799012 1.8260374]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "tf.Tensor(\n",
            "[[[3.6096988 2.9276514 2.1850147 3.0830379]\n",
            "  [3.5816612 2.9478528 2.197434  3.0901368]\n",
            "  [3.5807714 2.9463234 2.191193  3.0957341]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "tf.Tensor(\n",
            "[[[2.6692083 2.739544  2.7760735 3.4935157]\n",
            "  [2.7068827 2.7709622 2.7728791 3.4823875]\n",
            "  [2.6996639 2.7612586 2.7779686 3.4790032]]], shape=(1, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_out_b = tf.reshape(tf.transpose(all_heads_output, perm=[0, 2, 1, 3]),\n",
        "                            shape=(batch_size, seq_len, embed_dim))"
      ],
      "metadata": {
        "id": "DCaOeHKosHPx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.d_head = self.d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wk = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wv = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "    # Linear layer to generate the final output.\n",
        "    self.dense = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    split_inputs = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_head))\n",
        "    return tf.transpose(split_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def merge_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    merged_inputs = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    return tf.reshape(merged_inputs, (batch_size, -1, self.d_model))\n",
        "\n",
        "  def call(self, q, k, v, mask):\n",
        "    qs = self.wq(q)\n",
        "    ks = self.wk(k)\n",
        "    vs = self.wv(v)\n",
        "\n",
        "    qs = self.split_heads(qs)\n",
        "    ks = self.split_heads(ks)\n",
        "    vs = self.split_heads(vs)\n",
        "\n",
        "    output, attn_weights = scaled_dot_product_attention(qs, ks, vs, mask)\n",
        "    output = self.merge_heads(output)\n",
        "\n",
        "    return self.dense(output), attn_weights"
      ],
      "metadata": {
        "id": "by5vGEpFsItc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mhsa = MultiHeadSelfAttention(12, 3)\n",
        "\n",
        "output, attn_weights = mhsa(x, x, x, None)\n",
        "print(f\"MHSA output{output.shape}:\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV4l3TNysVKB",
        "outputId": "cd55d172-ab58-4b8c-c78a-a67d72f0bde2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MHSA output(1, 3, 12):\n",
            "tf.Tensor(\n",
            "[[[ 1.3627412   0.16063865  0.38592675  0.3214462   0.06935024\n",
            "    0.5341093   0.2784205   0.30460346 -0.88454527  0.2611329\n",
            "   -0.150915   -0.20781514]\n",
            "  [ 1.403109    0.14670819  0.4057802   0.33295926  0.05734877\n",
            "    0.55069447  0.28100032  0.29597065 -0.87676257  0.25711\n",
            "   -0.17546228 -0.21427244]\n",
            "  [ 1.3713204   0.16634938  0.38348597  0.3137713   0.07374293\n",
            "    0.5217956   0.29557145  0.2968211  -0.89511603  0.24521267\n",
            "   -0.15199354 -0.20717233]]], shape=(1, 3, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Block"
      ],
      "metadata": {
        "id": "xxZVTOkusbbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward_network(d_model, hidden_dim):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(hidden_dim, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "metadata": {
        "id": "r90Q0Q3KsXnZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "\n",
        "    self.mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
        "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    mhsa_output, attn_weights = self.mhsa(x, x, x, mask)\n",
        "    mhsa_output = self.dropout1(mhsa_output, training=training)\n",
        "    mhsa_output = self.layernorm1(x + mhsa_output)\n",
        "\n",
        "    ffn_output = self.ffn(mhsa_output)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    output = self.layernorm2(mhsa_output + ffn_output)\n",
        "\n",
        "    return output, attn_weights"
      ],
      "metadata": {
        "id": "Uh-8VTJ3sjtY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_block = EncoderBlock(12, 3, 48)\n",
        "\n",
        "block_output,  _ = encoder_block(x, True, None)\n",
        "print(f\"Output from single encoder block {block_output.shape}:\")\n",
        "print(block_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sUNrCkRsr2G",
        "outputId": "7ca3de24-2940-4d61-8f88-5d1a9e3b4cad"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from single encoder block (1, 3, 12):\n",
            "tf.Tensor(\n",
            "[[[ 0.8362456  -0.8239631   1.046871    0.81172305 -0.5489809\n",
            "    0.45544222  0.30277464  0.7323488  -1.7660596  -0.2322502\n",
            "    1.0452968  -1.8594481 ]\n",
            "  [-0.9280708  -0.03349714  1.4318237  -1.1032745  -0.6403576\n",
            "   -1.0067633   0.54416806  0.25722837  0.07079867  0.35319436\n",
            "    2.1678832  -1.1131334 ]\n",
            "  [-1.4087355   0.6905646   1.6501527   0.3185261  -1.369937\n",
            "    0.28188705 -0.24529329  0.9016651  -1.2333441  -0.88465244\n",
            "    0.04952613  1.2496405 ]]], shape=(1, 3, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word and Positional Embeddings"
      ],
      "metadata": {
        "id": "cO7xhmxhsxG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bpemb_en = BPEmb(lang=\"en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndTSf4gIswty",
        "outputId": "9a7eb4f4-4ef5-4cb2-9030-d8aa79b7143f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400869/400869 [00:00<00:00, 1060027.91B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3784656/3784656 [00:00<00:00, 5189426.11B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpemb_vocab_size, bpemb_embed_size = bpemb_en.vectors.shape\n",
        "print(\"Vocabulary size:\", bpemb_vocab_size)\n",
        "print(\"Embedding size:\", bpemb_embed_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k6lQKn2svX4",
        "outputId": "7bf58641-7410-456e-bb1c-1bba9934379d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10000\n",
            "Embedding size: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding for the word \"car\".\n",
        "bpemb_en.vectors[bpemb_en.words.index('car')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr3tCFoZs9Hu",
        "outputId": "9463d872-64a5-47a8-d56f-67794197e1de"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.305548, -0.325598, -0.134716, -0.078735, -0.660545,  0.076211,\n",
              "       -0.735487,  0.124533, -0.294402,  0.459688,  0.030137,  0.174041,\n",
              "       -0.224223,  0.486189, -0.504649, -0.459699,  0.315747,  0.477885,\n",
              "        0.091398,  0.427867,  0.016524, -0.076833, -0.899727,  0.493158,\n",
              "       -0.022309, -0.422785, -0.154148,  0.204981,  0.379834,  0.070588,\n",
              "        0.196073, -0.368222,  0.473406,  0.007409,  0.004303, -0.007823,\n",
              "       -0.19103 , -0.202509,  0.109878, -0.224521, -0.35741 , -0.611633,\n",
              "        0.329958, -0.212956, -0.497499, -0.393839, -0.130101, -0.216903,\n",
              "       -0.105595, -0.076007, -0.483942, -0.139704, -0.161647,  0.136985,\n",
              "        0.415363, -0.360143,  0.038601, -0.078804, -0.030421,  0.324129,\n",
              "        0.223378, -0.523636, -0.048317, -0.032248, -0.117367,  0.470519,\n",
              "        0.225816, -0.222065, -0.225007, -0.165904, -0.334389, -0.20157 ,\n",
              "        0.572352, -0.268794,  0.301929, -0.005563,  0.387491,  0.261031,\n",
              "       -0.11613 ,  0.074982, -0.008433,  0.259987, -0.099893, -0.268875,\n",
              "       -0.054047, -0.534776, -0.111101, -0.051742,  0.214114,  0.04293 ,\n",
              "        0.039873, -0.453112,  0.087382, -0.333201, -0.034079, -0.833045,\n",
              "        0.155232, -1.132393, -0.294766,  0.327572], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"Where can I find a placement?\"\n",
        "tokens = bpemb_en.encode(sample_sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59ptfgSOtBeY",
        "outputId": "600b8160-bf81-459b-b1d9-a0434f50331d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁where', '▁can', '▁i', '▁find', '▁a', '▁place', 'ment', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_seq = np.array(bpemb_en.encode_ids(\"Where can I find a placement?\"))\n",
        "print(token_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ychnhyBXtJJa",
        "outputId": "e4cc0b3e-269f-4038-b16c-69e33ce06b00"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 571  280  386 1934    4 1088  187 9967]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embed = tf.keras.layers.Embedding(bpemb_vocab_size, embed_dim)\n",
        "token_embeddings = token_embed(token_seq)\n",
        "\n",
        "# The untrained embeddings for our sample sentence.\n",
        "print(\"Embeddings for: \", sample_sentence)\n",
        "print(token_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGyTTekLtPzz",
        "outputId": "6d04cfa7-920d-4b3e-97b3-118c01a708b5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings for:  Where can I find a placement?\n",
            "tf.Tensor(\n",
            "[[ 9.43394750e-03  2.24299915e-02 -2.55576260e-02  2.87322067e-02\n",
            "   1.54048316e-02 -2.71322485e-02  2.13734396e-02  4.89125364e-02\n",
            "   1.16499662e-02 -1.41085312e-03 -2.12918166e-02  3.98954861e-02]\n",
            " [-7.93106481e-03  1.68977641e-02  3.32831256e-02 -7.79966265e-03\n",
            "  -9.99156386e-03  4.01916616e-02 -3.46181020e-02  4.90086712e-02\n",
            "  -1.90275908e-03  1.69362910e-02  1.92231201e-02  2.28811428e-03]\n",
            " [ 5.44165447e-03  3.08149345e-02  1.36735290e-03 -2.95903087e-02\n",
            "   2.68846750e-03 -7.74685293e-03  7.65486807e-03  3.18320282e-02\n",
            "  -1.42325982e-02  4.53356840e-02 -3.17296386e-02  2.14823596e-02]\n",
            " [-9.77023691e-03 -1.25811473e-02 -5.85842878e-04 -9.28838179e-03\n",
            "   4.91710417e-02 -4.36002016e-03 -1.06690750e-02 -4.43048738e-02\n",
            "   2.58824937e-02 -1.62694938e-02  1.19224191e-02  2.12631114e-02]\n",
            " [ 1.44097917e-02  2.43095271e-02 -2.08809506e-02  1.16189495e-02\n",
            "  -8.24720785e-03 -2.18522549e-03 -2.73436438e-02 -1.53779984e-05\n",
            "  -3.93536575e-02  3.56867574e-02 -4.96975295e-02  4.34296839e-02]\n",
            " [-4.66821454e-02  3.82520296e-02  1.30819418e-02 -1.25086308e-03\n",
            "  -7.26240873e-03 -4.85253446e-02  1.03638060e-02 -4.32512760e-02\n",
            "  -1.08621828e-02  3.14259790e-02  4.41167690e-02  3.61774452e-02]\n",
            " [-4.62268852e-02 -4.55894955e-02 -2.94496901e-02 -4.09757011e-02\n",
            "   2.81086601e-02  4.56451252e-03  4.90699746e-02 -4.64569777e-04\n",
            "  -1.15955584e-02 -7.79218599e-03  3.75611074e-02  1.82509087e-02]\n",
            " [ 6.69118017e-03  1.85977854e-02  2.79004313e-02  1.23454109e-02\n",
            "   3.55568565e-02 -1.90849304e-02 -4.36567441e-02  4.99034785e-02\n",
            "   9.46303457e-03  4.46084253e-02  3.03638242e-02  3.99587303e-03]], shape=(8, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 256\n",
        "pos_embed = tf.keras.layers.Embedding(max_seq_len, embed_dim)\n",
        "\n",
        "# Generate ids for each position of the token sequence.\n",
        "pos_idx = tf.range(len(token_seq))\n",
        "print(pos_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzEgQ3Qqtf3j",
        "outputId": "90a0d5b4-00f8-4bcd-93d7-699798d66f3b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6 7], shape=(8,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position_embeddings = pos_embed(pos_idx)\n",
        "print(\"Position embeddings for the input sequence\\n\", position_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmbRBKD4tqUs",
        "outputId": "1e5ae4d4-1b5b-4550-9ae6-ea0d68ec01b9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Position embeddings for the input sequence\n",
            " tf.Tensor(\n",
            "[[ 0.0182374  -0.04646475 -0.00375506 -0.01233542  0.03764436  0.00863379\n",
            "   0.00935394 -0.03585695  0.03879056  0.01691462 -0.01131745  0.011762  ]\n",
            " [-0.02997353  0.02893126  0.0034434   0.01245123 -0.00723519  0.01815319\n",
            "  -0.04335778 -0.01362966  0.01541758 -0.03164274  0.03858277 -0.00330187]\n",
            " [ 0.00508808  0.0148906  -0.00459822 -0.0311793   0.03297097  0.01159291\n",
            "  -0.00545932  0.00125635 -0.02360886  0.04261509  0.02811799  0.01085458]\n",
            " [-0.00920998 -0.03255661 -0.01375307  0.02883606 -0.03784255 -0.03116384\n",
            "  -0.01753619 -0.00243188 -0.01238028  0.01684532 -0.03331041  0.04496412]\n",
            " [-0.03949132 -0.00810491  0.01725994 -0.04239985 -0.02407693  0.01918412\n",
            "  -0.02115929 -0.00855763 -0.01861004 -0.03655253  0.00440925  0.01358545]\n",
            " [ 0.01651144  0.00551487  0.04656825  0.02754091  0.03595329  0.01630941\n",
            "  -0.00906213  0.03377352  0.01843799 -0.00877187  0.03260056 -0.0125886 ]\n",
            " [-0.02006437  0.03781242  0.03810674 -0.04497066 -0.0057422   0.01028169\n",
            "  -0.0025267   0.02379868 -0.01836564 -0.00429656  0.04904139 -0.0013984 ]\n",
            " [ 0.01132002  0.00390997  0.01653893  0.01055991 -0.02145556 -0.0437309\n",
            "  -0.03776897 -0.02717562  0.02820579 -0.0292279   0.00454372 -0.0338698 ]], shape=(8, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = token_embeddings + position_embeddings\n",
        "print(\"Input to the initial encoder block:\\n\", input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cb3OTGqts6z",
        "outputId": "2c3b0ed0-b99b-443c-9284-f0d96352357b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input to the initial encoder block:\n",
            " tf.Tensor(\n",
            "[[ 0.02767135 -0.02403476 -0.02931268  0.01639679  0.05304919 -0.01849846\n",
            "   0.03072738  0.01305559  0.05044052  0.01550376 -0.03260927  0.05165749]\n",
            " [-0.0379046   0.04582903  0.03672652  0.00465157 -0.01722676  0.05834486\n",
            "  -0.07797588  0.03537901  0.01351482 -0.01470644  0.05780589 -0.00101376]\n",
            " [ 0.01052973  0.04570553 -0.00323087 -0.06076961  0.03565944  0.00384606\n",
            "   0.00219555  0.03308837 -0.03784146  0.08795077 -0.00361165  0.03233694]\n",
            " [-0.01898022 -0.04513776 -0.01433891  0.01954768  0.01132849 -0.03552385\n",
            "  -0.02820526 -0.04673675  0.01350221  0.00057583 -0.02138799  0.06622723]\n",
            " [-0.02508153  0.01620462 -0.00362101 -0.0307809  -0.03232414  0.0169989\n",
            "  -0.04850294 -0.00857301 -0.0579637  -0.00086577 -0.04528828  0.05701514]\n",
            " [-0.0301707   0.0437669   0.05965019  0.02629005  0.02869089 -0.03221594\n",
            "   0.00130167 -0.00947776  0.0075758   0.0226541   0.07671732  0.02358885]\n",
            " [-0.06629125 -0.00777708  0.00865705 -0.08594636  0.02236646  0.01484621\n",
            "   0.04654327  0.02333411 -0.0299612  -0.01208875  0.0866025   0.01685251]\n",
            " [ 0.0180112   0.02250775  0.04443936  0.02290532  0.01410129 -0.06281583\n",
            "  -0.08142571  0.02272786  0.03766882  0.01538052  0.03490755 -0.02987393]], shape=(8, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "h70mjtOWtyMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, src_vocab_size,\n",
        "               max_seq_len, dropout_rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_embed = tf.keras.layers.Embedding(src_vocab_size, self.d_model)\n",
        "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
        "\n",
        "    # The original Attention Is All You Need paper applied dropout to the\n",
        "    # input before feeding it to the first encoder block.\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    # Create encoder blocks.\n",
        "    self.blocks = [EncoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate)\n",
        "    for _ in range(num_blocks)]\n",
        "\n",
        "  def call(self, input, training, mask):\n",
        "    token_embeds = self.token_embed(input)\n",
        "\n",
        "    # Generate position indices for a batch of input sequences.\n",
        "    num_pos = input.shape[0] * self.max_seq_len\n",
        "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "    pos_idx = np.reshape(pos_idx, input.shape)\n",
        "    pos_embeds = self.pos_embed(pos_idx)\n",
        "\n",
        "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
        "\n",
        "    # Run input through successive encoder blocks.\n",
        "    for block in self.blocks:\n",
        "      x, weights = block(x, training, mask)\n",
        "\n",
        "    return x, weights"
      ],
      "metadata": {
        "id": "XIzZNkmvtwrW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch of 3 sequences, each of length 10 (10 is also the\n",
        "# maximum sequence length in this case).\n",
        "seqs = np.random.randint(0, 10000, size=(3, 10))\n",
        "print(seqs.shape)\n",
        "print(seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBW_v7s7t7NA",
        "outputId": "e433a059-d671-417b-d892-32ac1d0eb05c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10)\n",
            "[[8911 4502 7570 5505 2915 7812 6777 1491  174 3097]\n",
            " [9789 5868 5632 6606   61 9679 1698 1771 2137 5813]\n",
            " [8107 3305 4311 8309 6582 7621 2868 5331 9619 3789]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ids = np.resize(np.arange(seqs.shape[1]), seqs.shape[0] * seqs.shape[1])\n",
        "print(pos_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUcQQACSup77",
        "outputId": "3d5524ec-139b-41dc-df5c-c7c7bcee6d53"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ids = np.reshape(pos_ids, (3, 10))\n",
        "print(pos_ids.shape)\n",
        "print(pos_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pC_FoK0uvwF",
        "outputId": "8ad70b57-e21b-4af1-9f8f-7fcb7e9f8fd7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10)\n",
            "[[0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embed(pos_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nou9crPMuy63",
        "outputId": "43b09302-3476-4e3f-bc9f-356bce434d45"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10, 12), dtype=float32, numpy=\n",
              "array([[[ 0.0182374 , -0.04646475, -0.00375506, -0.01233542,\n",
              "          0.03764436,  0.00863379,  0.00935394, -0.03585695,\n",
              "          0.03879056,  0.01691462, -0.01131745,  0.011762  ],\n",
              "        [-0.02997353,  0.02893126,  0.0034434 ,  0.01245123,\n",
              "         -0.00723519,  0.01815319, -0.04335778, -0.01362966,\n",
              "          0.01541758, -0.03164274,  0.03858277, -0.00330187],\n",
              "        [ 0.00508808,  0.0148906 , -0.00459822, -0.0311793 ,\n",
              "          0.03297097,  0.01159291, -0.00545932,  0.00125635,\n",
              "         -0.02360886,  0.04261509,  0.02811799,  0.01085458],\n",
              "        [-0.00920998, -0.03255661, -0.01375307,  0.02883606,\n",
              "         -0.03784255, -0.03116384, -0.01753619, -0.00243188,\n",
              "         -0.01238028,  0.01684532, -0.03331041,  0.04496412],\n",
              "        [-0.03949132, -0.00810491,  0.01725994, -0.04239985,\n",
              "         -0.02407693,  0.01918412, -0.02115929, -0.00855763,\n",
              "         -0.01861004, -0.03655253,  0.00440925,  0.01358545],\n",
              "        [ 0.01651144,  0.00551487,  0.04656825,  0.02754091,\n",
              "          0.03595329,  0.01630941, -0.00906213,  0.03377352,\n",
              "          0.01843799, -0.00877187,  0.03260056, -0.0125886 ],\n",
              "        [-0.02006437,  0.03781242,  0.03810674, -0.04497066,\n",
              "         -0.0057422 ,  0.01028169, -0.0025267 ,  0.02379868,\n",
              "         -0.01836564, -0.00429656,  0.04904139, -0.0013984 ],\n",
              "        [ 0.01132002,  0.00390997,  0.01653893,  0.01055991,\n",
              "         -0.02145556, -0.0437309 , -0.03776897, -0.02717562,\n",
              "          0.02820579, -0.0292279 ,  0.00454372, -0.0338698 ],\n",
              "        [ 0.03801287, -0.01142358,  0.02063402, -0.03732262,\n",
              "          0.01370097,  0.04169122,  0.0031813 , -0.02050204,\n",
              "         -0.04029774,  0.01248201,  0.03419359,  0.02552632],\n",
              "        [-0.0094205 ,  0.01889559,  0.02210717,  0.02863035,\n",
              "         -0.03142469, -0.02787802, -0.0258251 , -0.04529322,\n",
              "         -0.0340838 , -0.03529531, -0.00641395,  0.00809141]],\n",
              "\n",
              "       [[ 0.0182374 , -0.04646475, -0.00375506, -0.01233542,\n",
              "          0.03764436,  0.00863379,  0.00935394, -0.03585695,\n",
              "          0.03879056,  0.01691462, -0.01131745,  0.011762  ],\n",
              "        [-0.02997353,  0.02893126,  0.0034434 ,  0.01245123,\n",
              "         -0.00723519,  0.01815319, -0.04335778, -0.01362966,\n",
              "          0.01541758, -0.03164274,  0.03858277, -0.00330187],\n",
              "        [ 0.00508808,  0.0148906 , -0.00459822, -0.0311793 ,\n",
              "          0.03297097,  0.01159291, -0.00545932,  0.00125635,\n",
              "         -0.02360886,  0.04261509,  0.02811799,  0.01085458],\n",
              "        [-0.00920998, -0.03255661, -0.01375307,  0.02883606,\n",
              "         -0.03784255, -0.03116384, -0.01753619, -0.00243188,\n",
              "         -0.01238028,  0.01684532, -0.03331041,  0.04496412],\n",
              "        [-0.03949132, -0.00810491,  0.01725994, -0.04239985,\n",
              "         -0.02407693,  0.01918412, -0.02115929, -0.00855763,\n",
              "         -0.01861004, -0.03655253,  0.00440925,  0.01358545],\n",
              "        [ 0.01651144,  0.00551487,  0.04656825,  0.02754091,\n",
              "          0.03595329,  0.01630941, -0.00906213,  0.03377352,\n",
              "          0.01843799, -0.00877187,  0.03260056, -0.0125886 ],\n",
              "        [-0.02006437,  0.03781242,  0.03810674, -0.04497066,\n",
              "         -0.0057422 ,  0.01028169, -0.0025267 ,  0.02379868,\n",
              "         -0.01836564, -0.00429656,  0.04904139, -0.0013984 ],\n",
              "        [ 0.01132002,  0.00390997,  0.01653893,  0.01055991,\n",
              "         -0.02145556, -0.0437309 , -0.03776897, -0.02717562,\n",
              "          0.02820579, -0.0292279 ,  0.00454372, -0.0338698 ],\n",
              "        [ 0.03801287, -0.01142358,  0.02063402, -0.03732262,\n",
              "          0.01370097,  0.04169122,  0.0031813 , -0.02050204,\n",
              "         -0.04029774,  0.01248201,  0.03419359,  0.02552632],\n",
              "        [-0.0094205 ,  0.01889559,  0.02210717,  0.02863035,\n",
              "         -0.03142469, -0.02787802, -0.0258251 , -0.04529322,\n",
              "         -0.0340838 , -0.03529531, -0.00641395,  0.00809141]],\n",
              "\n",
              "       [[ 0.0182374 , -0.04646475, -0.00375506, -0.01233542,\n",
              "          0.03764436,  0.00863379,  0.00935394, -0.03585695,\n",
              "          0.03879056,  0.01691462, -0.01131745,  0.011762  ],\n",
              "        [-0.02997353,  0.02893126,  0.0034434 ,  0.01245123,\n",
              "         -0.00723519,  0.01815319, -0.04335778, -0.01362966,\n",
              "          0.01541758, -0.03164274,  0.03858277, -0.00330187],\n",
              "        [ 0.00508808,  0.0148906 , -0.00459822, -0.0311793 ,\n",
              "          0.03297097,  0.01159291, -0.00545932,  0.00125635,\n",
              "         -0.02360886,  0.04261509,  0.02811799,  0.01085458],\n",
              "        [-0.00920998, -0.03255661, -0.01375307,  0.02883606,\n",
              "         -0.03784255, -0.03116384, -0.01753619, -0.00243188,\n",
              "         -0.01238028,  0.01684532, -0.03331041,  0.04496412],\n",
              "        [-0.03949132, -0.00810491,  0.01725994, -0.04239985,\n",
              "         -0.02407693,  0.01918412, -0.02115929, -0.00855763,\n",
              "         -0.01861004, -0.03655253,  0.00440925,  0.01358545],\n",
              "        [ 0.01651144,  0.00551487,  0.04656825,  0.02754091,\n",
              "          0.03595329,  0.01630941, -0.00906213,  0.03377352,\n",
              "          0.01843799, -0.00877187,  0.03260056, -0.0125886 ],\n",
              "        [-0.02006437,  0.03781242,  0.03810674, -0.04497066,\n",
              "         -0.0057422 ,  0.01028169, -0.0025267 ,  0.02379868,\n",
              "         -0.01836564, -0.00429656,  0.04904139, -0.0013984 ],\n",
              "        [ 0.01132002,  0.00390997,  0.01653893,  0.01055991,\n",
              "         -0.02145556, -0.0437309 , -0.03776897, -0.02717562,\n",
              "          0.02820579, -0.0292279 ,  0.00454372, -0.0338698 ],\n",
              "        [ 0.03801287, -0.01142358,  0.02063402, -0.03732262,\n",
              "          0.01370097,  0.04169122,  0.0031813 , -0.02050204,\n",
              "         -0.04029774,  0.01248201,  0.03419359,  0.02552632],\n",
              "        [-0.0094205 ,  0.01889559,  0.02210717,  0.02863035,\n",
              "         -0.03142469, -0.02787802, -0.0258251 , -0.04529322,\n",
              "         -0.0340838 , -0.03529531, -0.00641395,  0.00809141]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ids = np.reshape(pos_ids, (3, 10))\n",
        "print(pos_ids.shape)\n",
        "print(pos_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvc5gG24u4dn",
        "outputId": "33b2e757-9555-4151-be44-f79571fc56ca"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10)\n",
            "[[0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embed(pos_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWx72HhBvDtP",
        "outputId": "84598bea-8e1c-436b-cfd4-c7ffcf50fe31"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10, 12), dtype=float32, numpy=\n",
              "array([[[ 0.0182374 , -0.04646475, -0.00375506, -0.01233542,\n",
              "          0.03764436,  0.00863379,  0.00935394, -0.03585695,\n",
              "          0.03879056,  0.01691462, -0.01131745,  0.011762  ],\n",
              "        [-0.02997353,  0.02893126,  0.0034434 ,  0.01245123,\n",
              "         -0.00723519,  0.01815319, -0.04335778, -0.01362966,\n",
              "          0.01541758, -0.03164274,  0.03858277, -0.00330187],\n",
              "        [ 0.00508808,  0.0148906 , -0.00459822, -0.0311793 ,\n",
              "          0.03297097,  0.01159291, -0.00545932,  0.00125635,\n",
              "         -0.02360886,  0.04261509,  0.02811799,  0.01085458],\n",
              "        [-0.00920998, -0.03255661, -0.01375307,  0.02883606,\n",
              "         -0.03784255, -0.03116384, -0.01753619, -0.00243188,\n",
              "         -0.01238028,  0.01684532, -0.03331041,  0.04496412],\n",
              "        [-0.03949132, -0.00810491,  0.01725994, -0.04239985,\n",
              "         -0.02407693,  0.01918412, -0.02115929, -0.00855763,\n",
              "         -0.01861004, -0.03655253,  0.00440925,  0.01358545],\n",
              "        [ 0.01651144,  0.00551487,  0.04656825,  0.02754091,\n",
              "          0.03595329,  0.01630941, -0.00906213,  0.03377352,\n",
              "          0.01843799, -0.00877187,  0.03260056, -0.0125886 ],\n",
              "        [-0.02006437,  0.03781242,  0.03810674, -0.04497066,\n",
              "         -0.0057422 ,  0.01028169, -0.0025267 ,  0.02379868,\n",
              "         -0.01836564, -0.00429656,  0.04904139, -0.0013984 ],\n",
              "        [ 0.01132002,  0.00390997,  0.01653893,  0.01055991,\n",
              "         -0.02145556, -0.0437309 , -0.03776897, -0.02717562,\n",
              "          0.02820579, -0.0292279 ,  0.00454372, -0.0338698 ],\n",
              "        [ 0.03801287, -0.01142358,  0.02063402, -0.03732262,\n",
              "          0.01370097,  0.04169122,  0.0031813 , -0.02050204,\n",
              "         -0.04029774,  0.01248201,  0.03419359,  0.02552632],\n",
              "        [-0.0094205 ,  0.01889559,  0.02210717,  0.02863035,\n",
              "         -0.03142469, -0.02787802, -0.0258251 , -0.04529322,\n",
              "         -0.0340838 , -0.03529531, -0.00641395,  0.00809141]],\n",
              "\n",
              "       [[ 0.0182374 , -0.04646475, -0.00375506, -0.01233542,\n",
              "          0.03764436,  0.00863379,  0.00935394, -0.03585695,\n",
              "          0.03879056,  0.01691462, -0.01131745,  0.011762  ],\n",
              "        [-0.02997353,  0.02893126,  0.0034434 ,  0.01245123,\n",
              "         -0.00723519,  0.01815319, -0.04335778, -0.01362966,\n",
              "          0.01541758, -0.03164274,  0.03858277, -0.00330187],\n",
              "        [ 0.00508808,  0.0148906 , -0.00459822, -0.0311793 ,\n",
              "          0.03297097,  0.01159291, -0.00545932,  0.00125635,\n",
              "         -0.02360886,  0.04261509,  0.02811799,  0.01085458],\n",
              "        [-0.00920998, -0.03255661, -0.01375307,  0.02883606,\n",
              "         -0.03784255, -0.03116384, -0.01753619, -0.00243188,\n",
              "         -0.01238028,  0.01684532, -0.03331041,  0.04496412],\n",
              "        [-0.03949132, -0.00810491,  0.01725994, -0.04239985,\n",
              "         -0.02407693,  0.01918412, -0.02115929, -0.00855763,\n",
              "         -0.01861004, -0.03655253,  0.00440925,  0.01358545],\n",
              "        [ 0.01651144,  0.00551487,  0.04656825,  0.02754091,\n",
              "          0.03595329,  0.01630941, -0.00906213,  0.03377352,\n",
              "          0.01843799, -0.00877187,  0.03260056, -0.0125886 ],\n",
              "        [-0.02006437,  0.03781242,  0.03810674, -0.04497066,\n",
              "         -0.0057422 ,  0.01028169, -0.0025267 ,  0.02379868,\n",
              "         -0.01836564, -0.00429656,  0.04904139, -0.0013984 ],\n",
              "        [ 0.01132002,  0.00390997,  0.01653893,  0.01055991,\n",
              "         -0.02145556, -0.0437309 , -0.03776897, -0.02717562,\n",
              "          0.02820579, -0.0292279 ,  0.00454372, -0.0338698 ],\n",
              "        [ 0.03801287, -0.01142358,  0.02063402, -0.03732262,\n",
              "          0.01370097,  0.04169122,  0.0031813 , -0.02050204,\n",
              "         -0.04029774,  0.01248201,  0.03419359,  0.02552632],\n",
              "        [-0.0094205 ,  0.01889559,  0.02210717,  0.02863035,\n",
              "         -0.03142469, -0.02787802, -0.0258251 , -0.04529322,\n",
              "         -0.0340838 , -0.03529531, -0.00641395,  0.00809141]],\n",
              "\n",
              "       [[ 0.0182374 , -0.04646475, -0.00375506, -0.01233542,\n",
              "          0.03764436,  0.00863379,  0.00935394, -0.03585695,\n",
              "          0.03879056,  0.01691462, -0.01131745,  0.011762  ],\n",
              "        [-0.02997353,  0.02893126,  0.0034434 ,  0.01245123,\n",
              "         -0.00723519,  0.01815319, -0.04335778, -0.01362966,\n",
              "          0.01541758, -0.03164274,  0.03858277, -0.00330187],\n",
              "        [ 0.00508808,  0.0148906 , -0.00459822, -0.0311793 ,\n",
              "          0.03297097,  0.01159291, -0.00545932,  0.00125635,\n",
              "         -0.02360886,  0.04261509,  0.02811799,  0.01085458],\n",
              "        [-0.00920998, -0.03255661, -0.01375307,  0.02883606,\n",
              "         -0.03784255, -0.03116384, -0.01753619, -0.00243188,\n",
              "         -0.01238028,  0.01684532, -0.03331041,  0.04496412],\n",
              "        [-0.03949132, -0.00810491,  0.01725994, -0.04239985,\n",
              "         -0.02407693,  0.01918412, -0.02115929, -0.00855763,\n",
              "         -0.01861004, -0.03655253,  0.00440925,  0.01358545],\n",
              "        [ 0.01651144,  0.00551487,  0.04656825,  0.02754091,\n",
              "          0.03595329,  0.01630941, -0.00906213,  0.03377352,\n",
              "          0.01843799, -0.00877187,  0.03260056, -0.0125886 ],\n",
              "        [-0.02006437,  0.03781242,  0.03810674, -0.04497066,\n",
              "         -0.0057422 ,  0.01028169, -0.0025267 ,  0.02379868,\n",
              "         -0.01836564, -0.00429656,  0.04904139, -0.0013984 ],\n",
              "        [ 0.01132002,  0.00390997,  0.01653893,  0.01055991,\n",
              "         -0.02145556, -0.0437309 , -0.03776897, -0.02717562,\n",
              "          0.02820579, -0.0292279 ,  0.00454372, -0.0338698 ],\n",
              "        [ 0.03801287, -0.01142358,  0.02063402, -0.03732262,\n",
              "          0.01370097,  0.04169122,  0.0031813 , -0.02050204,\n",
              "         -0.04029774,  0.01248201,  0.03419359,  0.02552632],\n",
              "        [-0.0094205 ,  0.01889559,  0.02210717,  0.02863035,\n",
              "         -0.03142469, -0.02787802, -0.0258251 , -0.04529322,\n",
              "         -0.0340838 , -0.03529531, -0.00641395,  0.00809141]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch = [\n",
        "    \"Where can I find a pizzeria?\",\n",
        "    \"Mass hysteria over listeria.\",\n",
        "    \"I ain't no circle back girl.\"\n",
        "]\n",
        "\n",
        "bpemb_en.encode(input_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2k8jIU4vGzJ",
        "outputId": "bbb08f41-17e1-42fd-9f0f-56817cf8a117"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['▁where', '▁can', '▁i', '▁find', '▁a', '▁p', 'iz', 'zer', 'ia', '?'],\n",
              " ['▁mass', '▁hy', 'ster', 'ia', '▁over', '▁l', 'ister', 'ia', '.'],\n",
              " ['▁i', '▁a', 'in', \"'\", 't', '▁no', '▁circle', '▁back', '▁girl', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_seqs = bpemb_en.encode_ids(input_batch)\n",
        "print(\"Vectorized inputs:\")\n",
        "input_seqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XosmMZcUvKbr",
        "outputId": "fa25ef84-9e82-43c0-f959-3053a1b0f9b2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized inputs:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[571, 280, 386, 1934, 4, 24, 248, 4339, 177, 9967],\n",
              " [1535, 1354, 1238, 177, 380, 43, 871, 177, 9935],\n",
              " [386, 4, 6, 9937, 9915, 467, 5410, 810, 3692, 9935]]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(input_seqs, padding=\"post\")\n",
        "print(\"Input to the encoder:\")\n",
        "print(padded_input_seqs.shape)\n",
        "print(padded_input_seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbGe1Lz3vNWg",
        "outputId": "79232e66-e46e-4cce-924d-90dd1418d85b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input to the encoder:\n",
            "(3, 10)\n",
            "[[ 571  280  386 1934    4   24  248 4339  177 9967]\n",
            " [1535 1354 1238  177  380   43  871  177 9935    0]\n",
            " [ 386    4    6 9937 9915  467 5410  810 3692 9935]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_mask = tf.cast(tf.math.not_equal(padded_input_seqs, 0), tf.float32)\n",
        "print(\"Input:\")\n",
        "print(padded_input_seqs, '\\n')\n",
        "print(\"Encoder mask:\")\n",
        "print(enc_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8q3JTYQvRRN",
        "outputId": "a0abd354-275f-40da-baf8-6e36405d7a41"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "[[ 571  280  386 1934    4   24  248 4339  177 9967]\n",
            " [1535 1354 1238  177  380   43  871  177 9935    0]\n",
            " [ 386    4    6 9937 9915  467 5410  810 3692 9935]] \n",
            "\n",
            "Encoder mask:\n",
            "tf.Tensor(\n",
            "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], shape=(3, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_mask = enc_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "enc_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb6z5QjPva56",
        "outputId": "0f16f518-a08a-4374-8297-c7cc9ba77173"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 10), dtype=float32, numpy=\n",
              "array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_blocks = 6\n",
        "\n",
        "# d_model is the embedding dimension used throughout.\n",
        "d_model = 12\n",
        "\n",
        "num_heads = 3\n",
        "\n",
        "# Feed-forward network hidden dimension width.\n",
        "ffn_hidden_dim = 48\n",
        "\n",
        "src_vocab_size = bpemb_vocab_size\n",
        "max_input_seq_len = padded_input_seqs.shape[1]\n",
        "\n",
        "encoder = Encoder(\n",
        "    num_encoder_blocks,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    ffn_hidden_dim,\n",
        "    src_vocab_size,\n",
        "    max_input_seq_len)"
      ],
      "metadata": {
        "id": "gj0EUsH4vcQy"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_output, attn_weights = encoder(padded_input_seqs, training=True,\n",
        "                                       mask=enc_mask)\n",
        "print(f\"Encoder output {encoder_output.shape}:\")\n",
        "print(encoder_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MWJlltWvfzB",
        "outputId": "73ecef10-85ed-4a24-fdd0-2df3c2f33315"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output (3, 10, 12):\n",
            "tf.Tensor(\n",
            "[[[ 1.24779928e+00  7.30878949e-01 -1.61388266e+00 -6.78592801e-01\n",
            "   -1.67193055e-01 -3.07976324e-02  1.07393157e+00 -4.28785384e-01\n",
            "    1.08274233e+00  1.18549526e+00 -1.13649237e+00 -1.26510322e+00]\n",
            "  [ 7.92409658e-01  5.13804853e-01 -1.45590436e+00 -8.35416198e-01\n",
            "   -7.06900716e-01 -9.22919154e-01  9.45003510e-01 -9.92063522e-01\n",
            "    1.13214505e+00  3.39080542e-01 -6.18596971e-01  1.80935740e+00]\n",
            "  [ 1.09155011e+00  3.98919016e-01 -2.32779312e+00 -5.71277261e-01\n",
            "    2.03561470e-01 -4.81406987e-01  3.07595849e-01 -5.27413130e-01\n",
            "    4.90076959e-01  7.86656499e-01 -9.48771179e-01  1.57830179e+00]\n",
            "  [ 1.37312162e+00  3.67170066e-01 -2.13820696e+00 -5.93859076e-01\n",
            "    5.76953053e-01 -5.63953280e-01  6.62697256e-01 -6.88541472e-01\n",
            "    3.07096869e-01  1.84277996e-01 -1.01258361e+00  1.52582741e+00]\n",
            "  [ 9.96322572e-01 -5.18519916e-02 -1.97636509e+00 -4.38284129e-01\n",
            "    3.06086689e-01 -9.71361339e-01 -6.85579002e-01  8.60698819e-02\n",
            "    5.55739582e-01  7.86341548e-01 -6.23887062e-01  2.01676846e+00]\n",
            "  [ 1.14441812e+00  1.06604449e-01 -2.11118722e+00 -6.43171132e-01\n",
            "    2.50226647e-01  6.09556794e-01  3.76488008e-02 -8.48894179e-01\n",
            "    2.80210644e-01  8.18340063e-01 -1.20550680e+00  1.56175375e+00]\n",
            "  [ 6.01992548e-01  4.81416970e-01 -1.99843836e+00 -3.96302223e-01\n",
            "    3.12610745e-01 -4.32691395e-01  2.06804454e-01 -1.20549405e+00\n",
            "    7.52159417e-01  1.03251147e+00 -1.02354562e+00  1.66897595e+00]\n",
            "  [ 9.61821258e-01  4.00603056e-01 -2.10432076e+00  5.34018636e-01\n",
            "   -6.84429631e-02 -3.54242712e-01  1.16578080e-01 -1.18641841e+00\n",
            "    7.44715333e-01  1.81018010e-01 -9.82701957e-01  1.75737274e+00]\n",
            "  [ 6.60712361e-01  7.89660335e-01 -1.20952213e+00 -1.16309869e+00\n",
            "   -5.67173898e-01 -2.96742022e-01  9.33916152e-01 -1.48831987e+00\n",
            "    7.05217361e-01  5.20271242e-01 -7.12963760e-01  1.82804334e+00]\n",
            "  [ 9.42350745e-01  3.47258627e-01 -2.12685728e+00 -6.70661271e-01\n",
            "    1.30789533e-01 -7.19559550e-01 -2.28392303e-01 -7.39519358e-01\n",
            "    1.36744559e+00  1.03444040e+00 -5.95157504e-01  1.25786257e+00]]\n",
            "\n",
            " [[ 1.08133399e+00 -4.13983703e-01 -1.78821468e+00 -1.03099692e+00\n",
            "   -1.68701530e-01 -9.29666281e-01  4.19117838e-01 -7.53286004e-01\n",
            "    1.42939961e+00  1.09960103e+00 -1.59945279e-01  1.21534181e+00]\n",
            "  [ 9.64319825e-01 -8.08788836e-02 -1.66870046e+00  8.48625183e-01\n",
            "   -1.69683754e-01 -6.13080323e-01 -1.19556296e+00 -7.69910812e-01\n",
            "    1.42821181e+00  5.63070893e-01 -7.77202785e-01  1.47079253e+00]\n",
            "  [ 1.26387691e+00 -2.08658054e-01 -1.97457874e+00 -1.02808058e+00\n",
            "   -2.30935708e-01  7.41311669e-01  1.32628143e-01 -8.25006008e-01\n",
            "    1.20921814e+00  2.79455245e-01 -7.63109207e-01  1.40387809e+00]\n",
            "  [ 1.18554509e+00  7.31073141e-01 -1.09673047e+00 -5.20973206e-01\n",
            "   -1.25037956e+00 -8.61849308e-01  6.55376613e-01 -1.21261394e+00\n",
            "    1.06241250e+00  5.38584411e-01 -7.67373204e-01  1.53692794e+00]\n",
            "  [ 8.39520574e-01 -3.65165174e-02 -2.45998994e-01  4.30563897e-01\n",
            "    2.32508987e-01 -5.44085264e-01 -1.55936277e+00 -1.20639646e+00\n",
            "    1.06556094e+00 -6.46258473e-01 -5.52688122e-01  2.22315192e+00]\n",
            "  [ 1.69160664e+00  7.46331036e-01 -1.20072114e+00 -7.65731156e-01\n",
            "   -6.95670426e-01 -1.13805771e+00  6.35677099e-01 -1.39103150e+00\n",
            "    1.12709463e+00  7.01625288e-01 -4.12914425e-01  7.01791644e-01]\n",
            "  [ 1.11781120e+00  2.31119264e-02 -1.31388354e+00  1.62965199e-03\n",
            "    4.46736395e-01 -2.46407077e-01 -8.30442607e-01  6.10360168e-02\n",
            "    1.74530983e+00  1.34574974e+00 -7.05559909e-01 -1.64509165e+00]\n",
            "  [ 8.53980899e-01  3.13382328e-01 -2.10603547e+00 -5.69586575e-01\n",
            "   -4.09515738e-01  4.26826179e-01 -2.08572581e-01 -7.11447716e-01\n",
            "    1.38731861e+00  5.13158619e-01 -1.00620711e+00  1.51669812e+00]\n",
            "  [ 1.40869474e+00 -5.50438344e-01 -2.17116666e+00 -6.48567140e-01\n",
            "    3.39402258e-01 -3.05614322e-01  9.08696532e-01 -4.85059142e-01\n",
            "    1.41552627e+00  4.42639410e-01 -8.97219718e-01  5.43105960e-01]\n",
            "  [ 1.04284501e+00 -7.12313712e-01 -1.51872575e+00 -2.54396588e-01\n",
            "    7.04346776e-01  5.83224595e-01 -9.20374766e-02 -1.73403597e+00\n",
            "    1.18542838e+00  3.30274373e-01 -8.96478355e-01  1.36186898e+00]]\n",
            "\n",
            " [[ 4.84385490e-01  6.32203147e-02 -1.65403605e+00 -6.59275949e-01\n",
            "   -2.76815534e-01 -1.34940374e+00  7.13995755e-01 -1.74687669e-01\n",
            "    1.93892276e+00  3.73546809e-01 -7.53835380e-01  1.29398334e+00]\n",
            "  [ 1.63525045e+00  2.42117167e-01 -1.68722558e+00 -2.74652690e-01\n",
            "   -3.76819819e-01 -7.55519688e-01  4.65147346e-01 -7.39601970e-01\n",
            "    1.24197507e+00  2.13474914e-01 -1.26068425e+00  1.29653931e+00]\n",
            "  [ 1.17621601e+00 -1.37768147e-05 -1.97840321e+00 -4.70897645e-01\n",
            "   -1.25166461e-01 -8.10565203e-02  9.78276730e-02 -6.09992385e-01\n",
            "    1.25497377e+00  9.06549573e-01 -1.43712592e+00  1.26708889e+00]\n",
            "  [ 1.30718410e+00 -9.55240503e-02 -2.02839899e+00 -4.73964095e-01\n",
            "   -2.23280892e-01 -7.42872000e-01  2.43279949e-01 -3.50772619e-01\n",
            "    1.44198763e+00  4.87061501e-01 -9.52260792e-01  1.38756037e+00]\n",
            "  [ 7.32679546e-01 -2.81084448e-01 -1.79210901e+00 -9.54096437e-01\n",
            "   -3.22686918e-02 -5.91349781e-01  5.64160764e-01  1.71063989e-01\n",
            "    1.89170170e+00  1.41245997e+00 -1.39152810e-01 -9.82004941e-01]\n",
            "  [ 1.21162057e+00  3.06665480e-01 -1.18012428e+00 -9.27268982e-01\n",
            "   -3.53317976e-01 -1.10419142e+00  5.99621952e-01 -1.08252120e+00\n",
            "    1.39603603e+00  2.12055117e-01 -7.39370346e-01  1.66079497e+00]\n",
            "  [ 7.59632826e-01 -2.17893153e-01 -1.76200759e+00 -8.43897939e-01\n",
            "   -4.69303459e-01  3.68202776e-01 -2.21782893e-01 -8.96329224e-01\n",
            "    1.38935781e+00  4.78422523e-01 -5.52708626e-01  1.96830678e+00]\n",
            "  [ 1.27899885e+00  4.02010493e-02 -1.54084158e+00 -4.79252517e-01\n",
            "   -4.37134922e-01 -6.72947288e-01 -1.59884676e-01 -1.10497022e+00\n",
            "    1.74014938e+00  6.25551343e-01 -6.90509915e-01  1.40064049e+00]\n",
            "  [ 6.61248922e-01 -7.55289197e-01 -1.71144104e+00  1.65138990e-01\n",
            "    1.29065454e-01  5.66336691e-01  6.26225293e-01 -1.13626504e+00\n",
            "    1.86593115e+00 -1.28958255e-01 -1.25091207e+00  9.68918979e-01]\n",
            "  [ 6.36209488e-01  1.03877962e+00  1.22045025e-01 -5.57141602e-01\n",
            "   -5.07789731e-01 -9.98713613e-01 -4.59010191e-02 -1.86330485e+00\n",
            "    1.37752914e+00 -4.89988178e-01 -4.70350206e-01  1.75862598e+00]]], shape=(3, 10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Block"
      ],
      "metadata": {
        "id": "Ekmuoobzvl6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "\n",
        "    self.mhsa1 = MultiHeadSelfAttention(d_model, num_heads)\n",
        "    self.mhsa2 = MultiHeadSelfAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  # Note the decoder block takes two masks. One for the first MHSA, another\n",
        "  # for the second MHSA.\n",
        "  def call(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
        "    mhsa_output1, attn_weights = self.mhsa1(target, target, target, decoder_mask)\n",
        "    mhsa_output1 = self.dropout1(mhsa_output1, training=training)\n",
        "    mhsa_output1 = self.layernorm1(mhsa_output1 + target)\n",
        "\n",
        "    mhsa_output2, attn_weights = self.mhsa2(mhsa_output1, encoder_output,\n",
        "                                            encoder_output,\n",
        "                                            memory_mask)\n",
        "    mhsa_output2 = self.dropout2(mhsa_output2, training=training)\n",
        "    mhsa_output2 = self.layernorm2(mhsa_output2 + mhsa_output1)\n",
        "\n",
        "    ffn_output = self.ffn(mhsa_output2)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    output = self.layernorm3(ffn_output + mhsa_output2)\n",
        "\n",
        "    return output, attn_weights"
      ],
      "metadata": {
        "id": "TWfQ5TWevlhY"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "               max_seq_len, dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_embed = tf.keras.layers.Embedding(target_vocab_size, self.d_model)\n",
        "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.blocks = [DecoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) for _ in range(num_blocks)]\n",
        "\n",
        "  def call(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
        "    token_embeds = self.token_embed(target)\n",
        "\n",
        "    # Generate position indices.\n",
        "    num_pos = target.shape[0] * self.max_seq_len\n",
        "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "    pos_idx = np.reshape(pos_idx, target.shape)\n",
        "\n",
        "    pos_embeds = self.pos_embed(pos_idx)\n",
        "\n",
        "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x, weights = block(encoder_output, x, training, decoder_mask, memory_mask)\n",
        "\n",
        "    return x, weights"
      ],
      "metadata": {
        "id": "9fQdU5jkvka1"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Made up values.\n",
        "target_input_seqs = [\n",
        "    [1, 652, 723, 123, 62],\n",
        "    [1, 25,  98, 129, 248, 215, 359, 249],\n",
        "    [1, 2369, 1259, 125, 486],\n",
        "]"
      ],
      "metadata": {
        "id": "IzkQTED92N_s"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_target_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(target_input_seqs, padding=\"post\")\n",
        "print(\"Padded target inputs to the decoder:\")\n",
        "print(padded_target_input_seqs.shape)\n",
        "print(padded_target_input_seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSb0b2Sy2R60",
        "outputId": "d4adfcb6-7100-4c9a-9b1e-aba066adfc5b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded target inputs to the decoder:\n",
            "(3, 8)\n",
            "[[   1  652  723  123   62    0    0    0]\n",
            " [   1   25   98  129  248  215  359  249]\n",
            " [   1 2369 1259  125  486    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dec_padding_mask = tf.cast(tf.math.not_equal(padded_target_input_seqs, 0), tf.float32)\n",
        "dec_padding_mask = dec_padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "print(dec_padding_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSxm2kun2Yja",
        "outputId": "c2a6acff-99ff-4975-9de9-7f732af83972"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[1. 1. 1. 1. 1. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
            "\n",
            "\n",
            " [[[1. 1. 1. 1. 1. 0. 0. 0.]]]], shape=(3, 1, 1, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_input_seq_len = padded_target_input_seqs.shape[1]\n",
        "look_ahead_mask = tf.linalg.band_part(tf.ones((target_input_seq_len,\n",
        "                                               target_input_seq_len)), -1, 0)\n",
        "print(look_ahead_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL7wI_9E2aPg",
        "outputId": "cce0c28d-28b3-4f10-8031-a9cb5fee56ab"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1.]], shape=(8, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dec_mask = tf.minimum(dec_padding_mask, look_ahead_mask)\n",
        "print(\"The decoder mask:\")\n",
        "print(dec_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wO5684r2dXi",
        "outputId": "567d8da3-da1c-41d0-b52a-e6b1b906e805"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The decoder mask:\n",
            "tf.Tensor(\n",
            "[[[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 1. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]]]], shape=(3, 1, 8, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(6, 12, 3, 48, 10000, 8)\n",
        "decoder_output, _ = decoder(encoder_output, padded_target_input_seqs,\n",
        "                            True, dec_mask, enc_mask)\n",
        "print(f\"Decoder output {decoder_output.shape}:\")\n",
        "print(decoder_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_tdbXWX2f1D",
        "outputId": "888a0b6f-957d-43b2-950f-836aa3c4ac0e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output (3, 8, 12):\n",
            "tf.Tensor(\n",
            "[[[-0.7125096  -0.62496865 -1.4808569   0.4712454   2.2094297\n",
            "    0.57263213 -0.05093424 -0.1588107   1.0184709   0.70690656\n",
            "   -0.9434271  -1.0071777 ]\n",
            "  [-1.185755   -1.0648152  -0.9661477   0.31135082  2.327546\n",
            "    0.6153856  -0.58132297 -0.5615083   0.7888386   0.6564204\n",
            "    0.48624778 -0.82624006]\n",
            "  [-1.7063954  -0.7058001  -0.81138045  0.52329344  2.289081\n",
            "    0.5627186  -0.1984774  -0.8172244   0.8392352   0.37076873\n",
            "    0.31830823 -0.66412765]\n",
            "  [-1.4678495  -1.4571699  -0.7922786   0.6117952   1.4807787\n",
            "    0.51156825  0.1094014  -0.82864296  1.3847101   0.83917934\n",
            "    0.45826626 -0.8497583 ]\n",
            "  [-1.172585   -1.3574504  -0.99402785  0.2974091   2.3469124\n",
            "    0.18784532 -0.13188699 -0.72790927  1.0011796   0.446131\n",
            "    0.49940932 -0.39502722]\n",
            "  [-1.3320968  -0.8318049  -1.0966734   0.58376837  2.3366857\n",
            "    0.17160138 -0.39159867 -0.6881039   0.7350832   0.7059242\n",
            "    0.5393458  -0.7321313 ]\n",
            "  [-1.5533653  -0.7105918  -0.8407288   0.4571889   2.4770927\n",
            "   -0.6392622   0.00981503 -0.45762292  0.81744623  0.643915\n",
            "    0.28643763 -0.49032444]\n",
            "  [-1.450738   -1.0304513  -0.98767966  0.25463223  2.584514\n",
            "    0.3338611   0.09656174 -0.20737255 -0.15832895  0.71449196\n",
            "    0.3693451  -0.518836  ]]\n",
            "\n",
            " [[-1.888449   -0.2810092  -1.2210051   0.95354235  1.555952\n",
            "   -0.13281831 -0.76458144 -0.75017715  0.9353897   1.0782107\n",
            "    0.56569284 -0.05074757]\n",
            "  [-1.0414901  -2.2587857  -0.19892403  0.24065122  1.8216562\n",
            "    0.41818923  0.58122915 -0.86651206  0.6498262   0.63679963\n",
            "    0.3913754  -0.3740155 ]\n",
            "  [-1.5464998  -1.0330877  -1.5192468   0.6245421   1.0190724\n",
            "    0.6392418  -0.8980396  -0.32199338  0.7787086   1.3601538\n",
            "    1.004996   -0.1078473 ]\n",
            "  [-1.6016389  -1.4968272  -0.9142406   0.27144653  1.7628096\n",
            "    0.5412459  -0.5151394  -0.22825715  0.706982    0.9831289\n",
            "    0.9414147  -0.4509244 ]\n",
            "  [-0.10587801 -2.0918953  -1.0529311  -0.10294833  1.5577713\n",
            "    0.38183114  0.2536974  -0.98081124  0.76955414  1.0714465\n",
            "    0.8913517  -0.5911881 ]\n",
            "  [-0.25842407 -1.4190118  -1.1117797   0.14534518  0.9519496\n",
            "    0.8733731  -1.2529957   0.03408559 -0.13309225  1.415321\n",
            "    1.62337    -0.8681409 ]\n",
            "  [-0.13713242 -1.6310437  -0.37001836 -0.35306937  1.6124085\n",
            "    1.3454193  -1.6378683  -0.83604366  0.5483892   0.46823785\n",
            "    0.84628344  0.14443731]\n",
            "  [-0.7040876  -2.0410817  -0.8281245  -0.1259939   1.768324\n",
            "    0.9616105  -0.04080266 -0.4621943   1.1340345   0.41808078\n",
            "    0.6288686  -0.70863354]]\n",
            "\n",
            " [[-0.79801625 -1.3678753  -0.99813426  0.92769575  2.0963678\n",
            "    0.76668406 -0.14026572 -1.3113309   0.74787754  0.48454988\n",
            "   -0.09282635 -0.31472653]\n",
            "  [-1.0621607  -1.2298602  -0.9898489   0.9272371   1.4001609\n",
            "    0.632822   -0.8080426  -1.1049359   1.3431545   0.9748172\n",
            "    0.46542507 -0.54876864]\n",
            "  [-0.8857499  -1.1541787  -1.3428963   0.79071504  2.1345084\n",
            "    0.8558563  -0.25334683 -0.30938733  1.0018698   0.12794182\n",
            "   -0.98624814  0.02091603]\n",
            "  [-0.7140551  -1.485943   -0.9735375   0.27081928  2.4941335\n",
            "    0.62212557 -0.32509056 -0.40201524 -0.1801897   0.7689544\n",
            "    0.5744327  -0.64963406]\n",
            "  [-0.6564517  -1.4105967  -1.0941626   0.659691    2.3966308\n",
            "    0.37480685 -0.25660554 -0.92527866  0.6478869   0.3925971\n",
            "    0.45928147 -0.5877986 ]\n",
            "  [-0.7005118  -1.3272096  -0.6387159   0.64949346  1.3597006\n",
            "    0.8960266  -1.7383016  -0.72857183  1.1891487   0.68196696\n",
            "    0.79306054 -0.4360861 ]\n",
            "  [-1.1457711  -1.4839907  -0.8547281   0.67932767  2.2235665\n",
            "    0.30359095 -0.13500716 -0.770863    0.60112756  0.6800494\n",
            "    0.5858062  -0.6831083 ]\n",
            "  [-0.94046175 -0.7423891  -1.7614826   0.58766085  2.0559673\n",
            "    0.6940184  -0.42525387 -0.5203056   1.0264349   0.47332928\n",
            "    0.3100908  -0.75760895]]], shape=(3, 8, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "weq96WhF2lOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, source_vocab_size,\n",
        "               target_vocab_size, max_input_len, max_target_len, dropout_rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_blocks, d_model, num_heads, hidden_dim, source_vocab_size,\n",
        "                           max_input_len, dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "                           max_target_len, dropout_rate)\n",
        "\n",
        "    # The final dense layer to generate logits from the decoder output.\n",
        "    self.output_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, input_seqs, target_input_seqs, training, encoder_mask,\n",
        "           decoder_mask, memory_mask):\n",
        "    encoder_output, encoder_attn_weights = self.encoder(input_seqs,\n",
        "                                                        training, encoder_mask)\n",
        "\n",
        "    decoder_output, decoder_attn_weights = self.decoder(encoder_output,\n",
        "                                                        target_input_seqs, training,\n",
        "                                                        decoder_mask, memory_mask)\n",
        "\n",
        "    return self.output_layer(decoder_output), encoder_attn_weights, decoder_attn_weights"
      ],
      "metadata": {
        "id": "6rw90Z1s2jZy"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_blocks = 6,\n",
        "    d_model = 12,\n",
        "    num_heads = 3,\n",
        "    hidden_dim = 48,\n",
        "    source_vocab_size = bpemb_vocab_size,\n",
        "    target_vocab_size = 7000, # made-up target vocab size.\n",
        "    max_input_len = padded_input_seqs.shape[1],\n",
        "    max_target_len = padded_target_input_seqs.shape[1])\n",
        "\n",
        "transformer_output, _, _ = transformer(padded_input_seqs,\n",
        "                                       padded_target_input_seqs, True,\n",
        "                                       enc_mask, dec_mask, memory_mask=enc_mask)\n",
        "print(f\"Transformer output {transformer_output.shape}:\")\n",
        "print(transformer_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzock2aK2slC",
        "outputId": "12026d4a-9faf-413f-c1a7-d9653c45adb9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer output (3, 8, 7000):\n",
            "tf.Tensor(\n",
            "[[[ 0.0395216   0.03916568  0.02582054 ...  0.01666976 -0.05993929\n",
            "    0.00690763]\n",
            "  [ 0.00943986 -0.03221751 -0.00229993 ...  0.07339201 -0.06860453\n",
            "   -0.01346374]\n",
            "  [ 0.02648464  0.00802982 -0.0167323  ...  0.06233544 -0.05515555\n",
            "    0.00070162]\n",
            "  ...\n",
            "  [ 0.02204489 -0.00191058 -0.00376367 ...  0.08343042 -0.09241083\n",
            "   -0.00951072]\n",
            "  [ 0.02156831 -0.00031531 -0.00594937 ...  0.06019064 -0.06924941\n",
            "   -0.00282104]\n",
            "  [-0.00968296 -0.00193681  0.01834356 ...  0.07306445 -0.05591359\n",
            "    0.04568645]]\n",
            "\n",
            " [[ 0.02646083 -0.09181534 -0.03285166 ...  0.05564085 -0.04139944\n",
            "   -0.03555582]\n",
            "  [ 0.04389931 -0.05536838 -0.03379606 ...  0.05632143 -0.01494565\n",
            "   -0.00276387]\n",
            "  [ 0.0375099  -0.07298164 -0.01804597 ...  0.0793379  -0.03941778\n",
            "   -0.01402597]\n",
            "  ...\n",
            "  [ 0.03313862 -0.09166637 -0.01659454 ...  0.0384437  -0.01532382\n",
            "   -0.01681272]\n",
            "  [ 0.03983209 -0.05074334 -0.01175103 ...  0.05605035 -0.02289294\n",
            "   -0.0122582 ]\n",
            "  [ 0.02742818 -0.10487369 -0.01274976 ...  0.04009271 -0.00604663\n",
            "   -0.03485323]]\n",
            "\n",
            " [[ 0.05055838 -0.01350048  0.00730224 ...  0.08985189 -0.04029727\n",
            "   -0.01235215]\n",
            "  [ 0.04104893 -0.09269323 -0.01497452 ...  0.07377809 -0.01515341\n",
            "   -0.06530838]\n",
            "  [ 0.0560854  -0.06966227  0.01684891 ...  0.05267398 -0.04152846\n",
            "   -0.04801576]\n",
            "  ...\n",
            "  [ 0.01493005 -0.0791366  -0.00417449 ...  0.06952535 -0.03825232\n",
            "   -0.01317769]\n",
            "  [ 0.02947235 -0.08783086 -0.01454918 ...  0.07827347 -0.02431761\n",
            "   -0.01054342]\n",
            "  [ 0.04723807 -0.10547726 -0.00545494 ...  0.06309526 -0.01525146\n",
            "   -0.03573536]]], shape=(3, 8, 7000), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}